{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "428101c9-683f-4c4a-b76c-ef23ec2071c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.source\")  # Create source schema if it doesn't exist\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.target\")  # Create target schema if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e9ce7b-1803-497e-9a5b-89a3e399e6b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scd1 = spark.sql(\"select * from samples.accuweather.forecast_hourly_metric\")\n",
    "scd1.write.mode(\"overwrite\").saveAsTable(\"workspace.source.scd1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e870a465-f04d-4579-a03c-b517ba3fca43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source = spark.read.table('workspace.source.scd1')\n",
    "source.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58d23700-9ecd-4421-ad0f-71d974bcedf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Load Data From Source and concatenate all columns into 'ConCatValue'\n",
    "source = source.withColumn('ConCatValue', F.concat_ws('', *source.columns))\n",
    "display(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6160293d-42fa-455f-841a-b5db70890fad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add IndCurrent, CreatedDate, and ModifiedDate columns\n",
    "source = source.withColumn(\"IndCurrent\", F.lit(1)) \\\n",
    "    .withColumn(\"CreatedDate\", F.current_timestamp()) \\\n",
    "    .withColumn(\"ModifiedDate\", F.current_timestamp())\n",
    "source.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdce0903-8540-4790-a05f-786934860560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.orderBy(F.monotonically_increasing_id())\n",
    "source = source.withColumn(\"storage_id\", F.row_number().over(window_spec))\n",
    "\n",
    "first_cols = [\"storage_id\"]\n",
    "other_cols = [col for col in source.columns if col not in first_cols]\n",
    "source = source.select(first_cols + other_cols)\n",
    "display(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e456afab-4b32-40c2-88c0-06c4d574f35e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate SHA-256 hash of concatenated column values and drop 'ConCatValue'\n",
    "source = source.withColumn(\"RowHash\", F.sha2(F.col(\"ConCatValue\"), 256)).drop('ConCatValue')\n",
    "display(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76604bd0-f826-456f-b369-d3d3a6405f8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#writing to the target schema  \n",
    "source.write.mode(\"append\").saveAsTable(\"workspace.target.scd1\")\n",
    "# Display data from the target_table schema\n",
    "\n",
    "target_df = spark.sql(\"SELECT * FROM workspace.target.scd1\") \n",
    "display(target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cef8701-0150-4aaf-a344-12702094448f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SourceTable='workspace.source.scd1'\n",
    "TargetTable='workspace.target.scd1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee8948c-0484-45ed-9a59-b57757554395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SourceDf=spark.read.table(SourceTable)  # Read source table into DataFrame\n",
    "TargetDf=spark.read.table(TargetTable)  # Read target table into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98995fac-3b92-49be-a091-057b92ca06e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SourceDf.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1357e148-d0b9-40aa-9302-e9ed69665183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the DataFrame to show only rows where '(\"latitude\") == \"22.36851\"'\n",
    "# Display the filtered DataFrame for inspection\n",
    "SourceDf.filter(col(\"latitude\") == \"22.36851\").display()\n",
    "\n",
    "# The 'city' value for rows with (\"latitude\") == \"22.36851\" is 'Pune'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c44b76-9dd5-4cc8-95a4-07ba46115321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Update the 'city_name' column in SourceDf:\n",
    "# For rows where \"latitude\" == \"22.36851\", set the 'city_name' value to 'Pune'.\n",
    "# For all other rows, retain the original 'city_name' value.\n",
    "SourceDf = SourceDf.withColumn(\n",
    "    \"city_name\",\n",
    "    when(col(\"latitude\") == \"22.36851\", \"Pune\").otherwise(col(\"city_name\"))\n",
    ")\n",
    "\n",
    "# Display rows where \"latitude\" == \"22.36851\" to verify the 'city_name' column update.\n",
    "SourceDf.filter(col(\"latitude\") == \"22.36851\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "582dc8f2-38e9-4ef4-949a-d4a29f7f990b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a hash key by concatenating all columns into a single string column 'RowHash'\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Concatenate all columns in 'source' DataFrame into 'RowHash'\n",
    "SourceDf = SourceDf.withColumn('RowHash', F.concat_ws('', *SourceDf.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70589478-0341-42b6-b122-42eb88efcad7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add three new columns to SourceDf:\n",
    "# 1. 'IndCurrent': Set to 1 for all rows, indicating the current/active record.\n",
    "# 2. 'CreatedDate': Set to the current timestamp, representing when the record was created.\n",
    "# 3. 'ModifiedDate': Set to the current timestamp, representing when the record was last modified.\n",
    "SourceDf = SourceDf.withColumn(\"IndCurrent\", F.lit(1)) \\\n",
    "    .withColumn(\"CreatedDate\", F.current_timestamp()) \\\n",
    "    .withColumn(\"ModifiedDate\", F.current_timestamp())\n",
    "SourceDf.display()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14934f4e-5fd8-48d0-995f-e7b33c8f06d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SourceDf.filter(col(\"latitude\") == \"22.36851\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD_Operation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}