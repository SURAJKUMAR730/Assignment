{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d80b82fc-c8bc-4297-a4a5-fca33cb48513",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Basic Operation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce668b19-a7d5-4dfb-bdd5-52662f670672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**Creating Schema**\n",
    "Creating Source and Target Schemas in Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "428101c9-683f-4c4a-b76c-ef23ec2071c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.source\")  # Create source schema if it doesn't exist\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.target\")  # Create target schema if it doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a64edef5-e2fe-4e1b-b634-0cdd58c55989",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Loading and Saving SCD1 Data Table**\n",
    "Read from AccuWeather Source and Overwrite to Workspace Source Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e9ce7b-1803-497e-9a5b-89a3e399e6b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scd1 = spark.sql(\"select * from samples.accuweather.forecast_hourly_metric\")\n",
    "scd1.write.mode(\"overwrite\").saveAsTable(\"workspace.source.scd1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc6d3f86-2936-408d-80a9-4b0b1ac77b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Read and Display SCD1 Source Table**\n",
    "Load Data from workspace.source.scd1 and Visualize Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e870a465-f04d-4579-a03c-b517ba3fca43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source = spark.read.table('workspace.source.scd1')\n",
    "source.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4664b3f7-25ce-43f9-9b5f-336deb745ef2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Concatenate All Columns into 'ConCatValue'**\n",
    "Transform Source Data by Merging All Columns into a Single Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58d23700-9ecd-4421-ad0f-71d974bcedf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Load Data From Source and concatenate all columns into 'ConCatValue'\n",
    "source = source.withColumn('ConCatValue', F.concat_ws('', *source.columns))\n",
    "display(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da272b4f-33ef-464c-83ec-755d1f25bf36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Add Metadata Columns to Source Data\n",
    "Include IndCurrent, CreatedDate, and ModifiedDate in the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc553e6a-990a-412e-b243-fdb1ecf0ade4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> \n",
    "These columns are metadata as they describe data, not business values:\n",
    "IndCurrent: Flags current active record.\n",
    "CreatedDate: Timestamp when inserted.\n",
    "ModifiedDate: Timestamp of last update.\n",
    "\n",
    "  Used for tracking, versioning, and auditing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6160293d-42fa-455f-841a-b5db70890fad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add IndCurrent, CreatedDate, and ModifiedDate columns\n",
    "source = source.withColumn(\"IndCurrent\", F.lit(1)) \\\n",
    "    .withColumn(\"CreatedDate\", F.current_timestamp()) \\\n",
    "    .withColumn(\"ModifiedDate\", F.current_timestamp())\n",
    "source.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "709b34b2-ba80-4249-b797-e56e70740bfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Add storage_id in Ascending Order\n",
    "Assign Unique Row Numbers and Place storage_id as the First Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdce0903-8540-4790-a05f-786934860560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.orderBy(F.monotonically_increasing_id())\n",
    "source = source.withColumn(\"storage_id\", F.row_number().over(window_spec))\n",
    "\n",
    "first_cols = [\"storage_id\"]\n",
    "other_cols = [col for col in source.columns if col not in first_cols]\n",
    "source = source.select(first_cols + other_cols)\n",
    "display(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6c4d90a-2fd5-4ef3-b257-c5ab8f309035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generate Row-Level Hash Using SHA-256\n",
    "Create RowHash from Concatenated Values and Drop ConCatValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb6c3dbf-9e51-41a3-bc12-5421a2b679b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "SHA-256 is a cryptographic hash function that generates a fixed 256-bit (64-character) hash value.\n",
    "It ensures data integrity by uniquely representing input data with a secure digital fingerprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e456afab-4b32-40c2-88c0-06c4d574f35e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate SHA-256 hash of concatenated column values and drop 'ConCatValue'\n",
    "source = source.withColumn(\"RowHash\", F.sha2(F.col(\"ConCatValue\"), 256)).drop('ConCatValue')\n",
    "display(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c1bc79b-4339-4fa8-84de-d7bf46fe9dfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3802e3c7-780a-49f5-816c-32a6877819f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write to Target Schema and Display Data\n",
    "Append Transformed Data to workspace.target.scd1 and View Contents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76604bd0-f826-456f-b369-d3d3a6405f8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#writing to the target schema  \n",
    "source.write.mode(\"append\").saveAsTable(\"workspace.target.scd1\")\n",
    "# Display data from the target_table schema\n",
    "\n",
    "target_df = spark.sql(\"SELECT * FROM workspace.target.scd1\") \n",
    "display(target_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0c296ba-6dc5-4306-8d68-3bc867af25f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SCD_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5946fd4f-1c69-41e8-8b49-70ff9b2d4ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define Source and Target Table Names\n",
    "Set SourceTable and TargetTable Variables for Reusability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cef8701-0150-4aaf-a344-12702094448f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SourceTable='workspace.source.scd1'\n",
    "TargetTable='workspace.target.scd1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7cae74c-ce86-429c-ae70-34407d179e0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Source and Target Tables into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee8948c-0484-45ed-9a59-b57757554395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SourceDf=spark.read.table(SourceTable)  # Read source table into DataFrame\n",
    "TargetDf=spark.read.table(TargetTable)  # Read target table into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98995fac-3b92-49be-a091-057b92ca06e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SourceDf.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f603e6f6-1fcc-4220-bc79-1e3a72832875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Filter Rows by Latitude and Inspect City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1357e148-d0b9-40aa-9302-e9ed69665183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the DataFrame to show only rows where '(\"latitude\") == \"22.36851\"'\n",
    "# Display the filtered DataFrame for inspection\n",
    "SourceDf.filter(col(\"latitude\") == \"22.36851\").display()\n",
    "\n",
    "# The 'city' value for rows with (\"latitude\") == \"22.36851\" is 'Pune'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b881fa44-773c-4fe3-833a-ce781615b08a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Update City Name Based on Latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c44b76-9dd5-4cc8-95a4-07ba46115321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Update the 'city_name' column in SourceDf:\n",
    "# For rows where \"latitude\" == \"22.36851\", set the 'city_name' value to 'Pune'.\n",
    "# For all other rows, retain the original 'city_name' value.\n",
    "SourceDf = SourceDf.withColumn(\n",
    "    \"city_name\",\n",
    "    when(col(\"latitude\") == \"22.36851\", \"Pune\").otherwise(col(\"city_name\"))\n",
    ")\n",
    "\n",
    "# Display rows where \"latitude\" == \"22.36851\" to verify the 'city_name' column update.\n",
    "SourceDf.filter(col(\"latitude\") == \"22.36851\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c41273cc-b875-4514-b69a-2ee6a2f6c7d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create RowHash by Concatenating All Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "582dc8f2-38e9-4ef4-949a-d4a29f7f990b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a hash key by concatenating all columns into a single string column 'RowHash'\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Concatenate all columns in 'source' DataFrame into 'RowHash'\n",
    "SourceDf = SourceDf.withColumn('RowHash', F.concat_ws('', *SourceDf.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "136257f6-c376-461a-b9b3-7b13dd014c6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Add Metadata Columns to SourceDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70589478-0341-42b6-b122-42eb88efcad7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add three new columns to SourceDf:\n",
    "# 1. 'IndCurrent': Set to 1 for all rows, indicating the current/active record.\n",
    "# 2. 'CreatedDate': Set to the current timestamp, representing when the record was created.\n",
    "# 3. 'ModifiedDate': Set to the current timestamp, representing when the record was last modified.\n",
    "SourceDf = SourceDf.withColumn(\"IndCurrent\", F.lit(1)) \\\n",
    "    .withColumn(\"CreatedDate\", F.current_timestamp()) \\\n",
    "    .withColumn(\"ModifiedDate\", F.current_timestamp())\n",
    "SourceDf.display()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14934f4e-5fd8-48d0-995f-e7b33c8f06d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SourceDf.filter(col(\"latitude\") == \"22.36851\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf611a14-d1ee-481b-a5b8-693b486c79b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Implement SCD Type 1 Using Delta Lake Merge\n",
    "Update Target Table with Latest Data Based on storage_id and RowHash Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "026fe158-1117-429f-b297-5cca1d5f488a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<delta.connect.tables.DeltaMergeBuilder at 0xff9df62b2e50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import current_timestamp, col\n",
    "\n",
    "# Static configuration\n",
    "table_name = \"workspace.target_table.sales\"\n",
    "key_column = \"storage_id\"\n",
    "timestamp_column = \"ModifiedDate\"\n",
    "hash_column = \"RowHash\"\n",
    "created_column = \"CreatedDate\"\n",
    "\n",
    "# Reference Delta table\n",
    "target_table = DeltaTable.forName(spark, table_name)\n",
    "\n",
    "# Aliases\n",
    "src = SourceDf.alias(\"src\")\n",
    "tgt = target_table.alias(\"tgt\")\n",
    "\n",
    "# Columns to update (exclude key, timestamp, and created date)\n",
    "columns_to_update = [\n",
    "    col_name for col_name in SourceDf.columns \n",
    "    if col_name not in [key_column, timestamp_column, created_column]\n",
    "]\n",
    "\n",
    "# Construct SET dictionary for update\n",
    "set_dict = {col_name: col(f\"src.{col_name}\") for col_name in columns_to_update}\n",
    "set_dict[timestamp_column] = current_timestamp()  # Add ModifiedDate explicitly\n",
    "\n",
    "# Perform SCD Type 1 MERGE\n",
    "tgt.merge(\n",
    "    src,\n",
    "    f\"tgt.{key_column} = src.{key_column}\"\n",
    ").whenMatchedUpdate(\n",
    "    condition=col(f\"src.{hash_column}\") != col(f\"tgt.{hash_column}\"),\n",
    "    set=set_dict\n",
    ").whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bee0d528-6976-4773-b1f2-b00aadea1079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Query Target Table to Verify SCD1 Merge for Specific Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "739667ad-f14b-4472-a9c2-db9bc3e27c70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"select * from workspace.target.scd1 where storage_id = '1' \"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD 1_Operation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}