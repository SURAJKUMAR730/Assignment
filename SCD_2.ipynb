{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2e8ea59-d74f-4f8e-81bf-2b7f1ee17f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.source\")  # Create source schema if it doesn't exist\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS workspace.target\")  # Create target schema if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3257ee0d-8d55-4011-967f-1cc1ecb9ba4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales2 = spark.sql(\"select * from samples.bakehouse.sales_suppliers\")\n",
    "sales2.write.mode(\"overwrite\").saveAsTable(\"workspace.source.sales2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d80f124e-1211-419c-912f-4f3e2e670cc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>supplierID</th><th>name</th><th>ingredient</th><th>continent</th><th>city</th><th>district</th><th>size</th><th>longitude</th><th>latitude</th><th>approved</th></tr></thead><tbody><tr><td>4000000</td><td>Cacao Wonders</td><td>cacao</td><td>South America</td><td>Guayaquil</td><td>Las Peñas</td><td>M</td><td>-79.8974</td><td>-2.1791</td><td>Y</td></tr><tr><td>4000001</td><td>Coconut Grove</td><td>coconut</td><td>Asia</td><td>Manila</td><td>Intramuros</td><td>S</td><td>121.0221</td><td>14.6042</td><td>Y</td></tr><tr><td>4000002</td><td>Almond Delights</td><td>almonds</td><td>Europe</td><td>Valencia</td><td>Ruzafa</td><td>L</td><td>-0.3762</td><td>39.4699</td><td>Y</td></tr><tr><td>4000003</td><td>Sugar Cane Harvest</td><td>cane sugar</td><td>South America</td><td>Sao Paulo</td><td>Vila Madalena</td><td>XL</td><td>-46.6333</td><td>-23.5489</td><td>Y</td></tr><tr><td>4000004</td><td>Vanilla Valley</td><td>vanilla</td><td>North America</td><td>Mexico City</td><td>Roma Norte</td><td>M</td><td>-99.1332</td><td>19.4326</td><td>Y</td></tr><tr><td>4000005</td><td>Pecan Pleasures</td><td>pecans</td><td>North America</td><td>Atlanta</td><td>Virginia-Highland</td><td>S</td><td>-84.3888</td><td>33.749</td><td>Y</td></tr><tr><td>4000006</td><td>Hazelnut Haven</td><td>hazelnuts</td><td>Europe</td><td>Istanbul</td><td>Kadıköy</td><td>XXL</td><td>28.9784</td><td>41.0082</td><td>Y</td></tr><tr><td>4000007</td><td>Cinnamon Spice</td><td>cinnamon</td><td>Asia</td><td>Colombo</td><td>Galle Face Green</td><td>L</td><td>79.8612</td><td>6.9271</td><td>Y</td></tr><tr><td>4000008</td><td>Cashew Corner</td><td>cashews</td><td>Asia</td><td>Goa</td><td>Anjuna Beach</td><td>XL</td><td>73.8067</td><td>15.3173</td><td>Y</td></tr><tr><td>4000009</td><td>Maple Monarch</td><td>maple syrup</td><td>North America</td><td>Montreal</td><td>Plateau Mont-Royal</td><td>M</td><td>-73.5673</td><td>45.5017</td><td>Y</td></tr><tr><td>4000010</td><td>Pistachio Palace</td><td>pistachios</td><td>Asia</td><td>Tehran</td><td>Tajrish Bazaar</td><td>S</td><td>51.4215</td><td>35.7106</td><td>Y</td></tr><tr><td>4000011</td><td>Oat Oasis</td><td>oats</td><td>Europe</td><td>Edinburgh</td><td>Stockbridge</td><td>L</td><td>-3.1883</td><td>55.9533</td><td>Y</td></tr><tr><td>4000012</td><td>Coffee Collective</td><td>coffee</td><td>South America</td><td>Medellin</td><td>El Poblado</td><td>XXL</td><td>-75.5638</td><td>6.2518</td><td>Y</td></tr><tr><td>4000013</td><td>Ginger Gems</td><td>ginger</td><td>Asia</td><td>Chennai</td><td>Mylapore</td><td>M</td><td>80.2707</td><td>13.0827</td><td>Y</td></tr><tr><td>4000014</td><td>Molasses Mills</td><td>molasses</td><td>Central America</td><td>Havana</td><td>Vedado</td><td>XL</td><td>-82.3665</td><td>23.1136</td><td>Y</td></tr><tr><td>4000015</td><td>Honey Hives</td><td>honey</td><td>Africa</td><td>Addis Ababa</td><td>Piazza</td><td>S</td><td>38.7575</td><td>9.0084</td><td>Y</td></tr><tr><td>4000016</td><td>Sesame Seeds</td><td>sesame seeds</td><td>Asia</td><td>Yangon</td><td>Chinatown</td><td>L</td><td>96.1611</td><td>16.8409</td><td>Y</td></tr><tr><td>4000017</td><td>Peanut Plantation</td><td>peanuts</td><td>Africa</td><td>Kano</td><td>Kurmi Market</td><td>M</td><td>8.5167</td><td>12.0022</td><td>Y</td></tr><tr><td>4000018</td><td>Raisin Ranch</td><td>raisins</td><td>Asia</td><td>Kabul</td><td>Chicken Street</td><td>XXL</td><td>69.1763</td><td>34.521</td><td>Y</td></tr><tr><td>4000019</td><td>Cardamom Cove</td><td>cardamom</td><td>Asia</td><td>Kozhikode</td><td>Mappila Bay</td><td>XL</td><td>75.7804</td><td>11.2588</td><td>Y</td></tr><tr><td>4000020</td><td>Clove Cliffs</td><td>cloves</td><td>Africa</td><td>Zanzibar</td><td>Stone Town</td><td>S</td><td>39.1918</td><td>-6.167</td><td>Y</td></tr><tr><td>4000021</td><td>Cocoa Crops</td><td>cocoa butter</td><td>South America</td><td>Caracas</td><td>Las Mercedes</td><td>M</td><td>-66.9036</td><td>10.4806</td><td>Y</td></tr><tr><td>4000022</td><td>Poppy Peaks</td><td>poppy seeds</td><td>Europe</td><td>Krakow</td><td>Kazimierz</td><td>S</td><td>19.9368</td><td>50.0647</td><td>Y</td></tr><tr><td>4000023</td><td>Fennel Fields</td><td>fennel seeds</td><td>Europe</td><td>Florence</td><td>Santo Spirito</td><td>L</td><td>11.2558</td><td>43.7695</td><td>Y</td></tr><tr><td>4000024</td><td>Anise Acres</td><td>anise</td><td>Asia</td><td>Izmir</td><td>Alsancak</td><td>XL</td><td>27.1384</td><td>38.4237</td><td>Y</td></tr><tr><td>4000025</td><td>Nutmeg Nirvana</td><td>nutmeg</td><td>Asia</td><td>Banda Aceh</td><td>Peunayong</td><td>M</td><td>95.3198</td><td>5.5577</td><td>Y</td></tr><tr><td>4000026</td><td>Mace Meadows</td><td>mace</td><td>Asia</td><td>Ambon</td><td>Namalatu</td><td>S</td><td>128.1753</td><td>-3.6953</td><td>Y</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         4000000,
         "Cacao Wonders",
         "cacao",
         "South America",
         "Guayaquil",
         "Las Peñas",
         "M",
         -79.8974,
         -2.1791,
         "Y"
        ],
        [
         4000001,
         "Coconut Grove",
         "coconut",
         "Asia",
         "Manila",
         "Intramuros",
         "S",
         121.0221,
         14.6042,
         "Y"
        ],
        [
         4000002,
         "Almond Delights",
         "almonds",
         "Europe",
         "Valencia",
         "Ruzafa",
         "L",
         -0.3762,
         39.4699,
         "Y"
        ],
        [
         4000003,
         "Sugar Cane Harvest",
         "cane sugar",
         "South America",
         "Sao Paulo",
         "Vila Madalena",
         "XL",
         -46.6333,
         -23.5489,
         "Y"
        ],
        [
         4000004,
         "Vanilla Valley",
         "vanilla",
         "North America",
         "Mexico City",
         "Roma Norte",
         "M",
         -99.1332,
         19.4326,
         "Y"
        ],
        [
         4000005,
         "Pecan Pleasures",
         "pecans",
         "North America",
         "Atlanta",
         "Virginia-Highland",
         "S",
         -84.3888,
         33.749,
         "Y"
        ],
        [
         4000006,
         "Hazelnut Haven",
         "hazelnuts",
         "Europe",
         "Istanbul",
         "Kadıköy",
         "XXL",
         28.9784,
         41.0082,
         "Y"
        ],
        [
         4000007,
         "Cinnamon Spice",
         "cinnamon",
         "Asia",
         "Colombo",
         "Galle Face Green",
         "L",
         79.8612,
         6.9271,
         "Y"
        ],
        [
         4000008,
         "Cashew Corner",
         "cashews",
         "Asia",
         "Goa",
         "Anjuna Beach",
         "XL",
         73.8067,
         15.3173,
         "Y"
        ],
        [
         4000009,
         "Maple Monarch",
         "maple syrup",
         "North America",
         "Montreal",
         "Plateau Mont-Royal",
         "M",
         -73.5673,
         45.5017,
         "Y"
        ],
        [
         4000010,
         "Pistachio Palace",
         "pistachios",
         "Asia",
         "Tehran",
         "Tajrish Bazaar",
         "S",
         51.4215,
         35.7106,
         "Y"
        ],
        [
         4000011,
         "Oat Oasis",
         "oats",
         "Europe",
         "Edinburgh",
         "Stockbridge",
         "L",
         -3.1883,
         55.9533,
         "Y"
        ],
        [
         4000012,
         "Coffee Collective",
         "coffee",
         "South America",
         "Medellin",
         "El Poblado",
         "XXL",
         -75.5638,
         6.2518,
         "Y"
        ],
        [
         4000013,
         "Ginger Gems",
         "ginger",
         "Asia",
         "Chennai",
         "Mylapore",
         "M",
         80.2707,
         13.0827,
         "Y"
        ],
        [
         4000014,
         "Molasses Mills",
         "molasses",
         "Central America",
         "Havana",
         "Vedado",
         "XL",
         -82.3665,
         23.1136,
         "Y"
        ],
        [
         4000015,
         "Honey Hives",
         "honey",
         "Africa",
         "Addis Ababa",
         "Piazza",
         "S",
         38.7575,
         9.0084,
         "Y"
        ],
        [
         4000016,
         "Sesame Seeds",
         "sesame seeds",
         "Asia",
         "Yangon",
         "Chinatown",
         "L",
         96.1611,
         16.8409,
         "Y"
        ],
        [
         4000017,
         "Peanut Plantation",
         "peanuts",
         "Africa",
         "Kano",
         "Kurmi Market",
         "M",
         8.5167,
         12.0022,
         "Y"
        ],
        [
         4000018,
         "Raisin Ranch",
         "raisins",
         "Asia",
         "Kabul",
         "Chicken Street",
         "XXL",
         69.1763,
         34.521,
         "Y"
        ],
        [
         4000019,
         "Cardamom Cove",
         "cardamom",
         "Asia",
         "Kozhikode",
         "Mappila Bay",
         "XL",
         75.7804,
         11.2588,
         "Y"
        ],
        [
         4000020,
         "Clove Cliffs",
         "cloves",
         "Africa",
         "Zanzibar",
         "Stone Town",
         "S",
         39.1918,
         -6.167,
         "Y"
        ],
        [
         4000021,
         "Cocoa Crops",
         "cocoa butter",
         "South America",
         "Caracas",
         "Las Mercedes",
         "M",
         -66.9036,
         10.4806,
         "Y"
        ],
        [
         4000022,
         "Poppy Peaks",
         "poppy seeds",
         "Europe",
         "Krakow",
         "Kazimierz",
         "S",
         19.9368,
         50.0647,
         "Y"
        ],
        [
         4000023,
         "Fennel Fields",
         "fennel seeds",
         "Europe",
         "Florence",
         "Santo Spirito",
         "L",
         11.2558,
         43.7695,
         "Y"
        ],
        [
         4000024,
         "Anise Acres",
         "anise",
         "Asia",
         "Izmir",
         "Alsancak",
         "XL",
         27.1384,
         38.4237,
         "Y"
        ],
        [
         4000025,
         "Nutmeg Nirvana",
         "nutmeg",
         "Asia",
         "Banda Aceh",
         "Peunayong",
         "M",
         95.3198,
         5.5577,
         "Y"
        ],
        [
         4000026,
         "Mace Meadows",
         "mace",
         "Asia",
         "Ambon",
         "Namalatu",
         "S",
         128.1753,
         -3.6953,
         "Y"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "supplierID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ingredient",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "continent",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "district",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "longitude",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "latitude",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "approved",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "source = spark.read.table('workspace.source.sales2')\n",
    "source.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "381b8a6d-710e-401c-9e1f-906b8e861e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a hash key by concatenating all columns into a single string column 'RowHash'\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Concatenate all columns in 'source' DataFrame into 'RowHash'\n",
    "source = source.withColumn('RowHash', F.sha2(F.concat_ws('', *source.columns), 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a78b6d50-945d-462e-ab38-2f84a22e8baf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>supplierID</th><th>name</th><th>ingredient</th><th>continent</th><th>city</th><th>district</th><th>size</th><th>longitude</th><th>latitude</th><th>approved</th><th>RowHash</th></tr></thead><tbody><tr><td>4000000</td><td>Cacao Wonders</td><td>cacao</td><td>South America</td><td>Guayaquil</td><td>Las Peñas</td><td>M</td><td>-79.8974</td><td>-2.1791</td><td>Y</td><td>19e37f9076246ae6ffb69e9be7ed2073d94656fb55aadc78ee09807ffd86ff36</td></tr><tr><td>4000001</td><td>Coconut Grove</td><td>coconut</td><td>Asia</td><td>Manila</td><td>Intramuros</td><td>S</td><td>121.0221</td><td>14.6042</td><td>Y</td><td>dd8379700727ada5448616395cee167291078ba63a10ae62f20ea3708b19004a</td></tr><tr><td>4000002</td><td>Almond Delights</td><td>almonds</td><td>Europe</td><td>Valencia</td><td>Ruzafa</td><td>L</td><td>-0.3762</td><td>39.4699</td><td>Y</td><td>2618ef1944d020b72ae94ce0343b054c4818855400650829c903c0a0bec5004c</td></tr><tr><td>4000003</td><td>Sugar Cane Harvest</td><td>cane sugar</td><td>South America</td><td>Sao Paulo</td><td>Vila Madalena</td><td>XL</td><td>-46.6333</td><td>-23.5489</td><td>Y</td><td>98327b4f32a42f832fa286d6fd6c96ba0734010fc2fdb4cb3ac62aa321f2b06c</td></tr><tr><td>4000004</td><td>Vanilla Valley</td><td>vanilla</td><td>North America</td><td>Mexico City</td><td>Roma Norte</td><td>M</td><td>-99.1332</td><td>19.4326</td><td>Y</td><td>e6d6a60d578c4b2907df80d8b136e67370b976e9a0754f68a4dd914058b61839</td></tr><tr><td>4000005</td><td>Pecan Pleasures</td><td>pecans</td><td>North America</td><td>Atlanta</td><td>Virginia-Highland</td><td>S</td><td>-84.3888</td><td>33.749</td><td>Y</td><td>eb24f9d60f76123cd4744b9bff18c66b5c1babd2a3950faef75a353837c96dd0</td></tr><tr><td>4000006</td><td>Hazelnut Haven</td><td>hazelnuts</td><td>Europe</td><td>Istanbul</td><td>Kadıköy</td><td>XXL</td><td>28.9784</td><td>41.0082</td><td>Y</td><td>432fd8759f3212a0b0fb1b90a232b03f65ff6883d1cf4247f71366737d55d732</td></tr><tr><td>4000007</td><td>Cinnamon Spice</td><td>cinnamon</td><td>Asia</td><td>Colombo</td><td>Galle Face Green</td><td>L</td><td>79.8612</td><td>6.9271</td><td>Y</td><td>5bb8fa1ebe1c59e25cf6022593adba52ad61c9fb79d47eccd81d402afa43597c</td></tr><tr><td>4000008</td><td>Cashew Corner</td><td>cashews</td><td>Asia</td><td>Goa</td><td>Anjuna Beach</td><td>XL</td><td>73.8067</td><td>15.3173</td><td>Y</td><td>edfa576dd2e0c945b119604b91e8d4680a64e069945367ff633b7ae2d72be8fe</td></tr><tr><td>4000009</td><td>Maple Monarch</td><td>maple syrup</td><td>North America</td><td>Montreal</td><td>Plateau Mont-Royal</td><td>M</td><td>-73.5673</td><td>45.5017</td><td>Y</td><td>2de584aa231a74956a5ac5dfd69a06aab3050410b11f8c89c095b95b4feaee24</td></tr><tr><td>4000010</td><td>Pistachio Palace</td><td>pistachios</td><td>Asia</td><td>Tehran</td><td>Tajrish Bazaar</td><td>S</td><td>51.4215</td><td>35.7106</td><td>Y</td><td>a7afea4951a2aa7bf2b6e58ecd8b712c44fd0e9d659004e47a1a9bc03ad6e356</td></tr><tr><td>4000011</td><td>Oat Oasis</td><td>oats</td><td>Europe</td><td>Edinburgh</td><td>Stockbridge</td><td>L</td><td>-3.1883</td><td>55.9533</td><td>Y</td><td>6ef1ac8bb1bd9456c4817028fc2d038187bc3577e9a62a0f1386d949b127ba24</td></tr><tr><td>4000012</td><td>Coffee Collective</td><td>coffee</td><td>South America</td><td>Medellin</td><td>El Poblado</td><td>XXL</td><td>-75.5638</td><td>6.2518</td><td>Y</td><td>4b60aa6f629df63cc6d694ea0213d6de5a03cd5c849243b953474ef81a2199c3</td></tr><tr><td>4000013</td><td>Ginger Gems</td><td>ginger</td><td>Asia</td><td>Chennai</td><td>Mylapore</td><td>M</td><td>80.2707</td><td>13.0827</td><td>Y</td><td>d0fdd754477596380fd0c9b3bfb4f3b5b3f184b4cc527131e3d51185e19ed686</td></tr><tr><td>4000014</td><td>Molasses Mills</td><td>molasses</td><td>Central America</td><td>Havana</td><td>Vedado</td><td>XL</td><td>-82.3665</td><td>23.1136</td><td>Y</td><td>fed099934e5785df6b1b7ffa7eb61a9cd02fefa6c13aac2da93a82348d68f361</td></tr><tr><td>4000015</td><td>Honey Hives</td><td>honey</td><td>Africa</td><td>Addis Ababa</td><td>Piazza</td><td>S</td><td>38.7575</td><td>9.0084</td><td>Y</td><td>10d4b53538fc1b2296542295ba141067f8c7198424daa003a9d434c2a262da83</td></tr><tr><td>4000016</td><td>Sesame Seeds</td><td>sesame seeds</td><td>Asia</td><td>Yangon</td><td>Chinatown</td><td>L</td><td>96.1611</td><td>16.8409</td><td>Y</td><td>424d71a18a50a22e7e4afb5aa3546bb1ae81cba3a4ecd8981d30ca20b283c65c</td></tr><tr><td>4000017</td><td>Peanut Plantation</td><td>peanuts</td><td>Africa</td><td>Kano</td><td>Kurmi Market</td><td>M</td><td>8.5167</td><td>12.0022</td><td>Y</td><td>06a6e7680bcc613529013a1c61f185f36216600157af9706c3acb33d17c307e5</td></tr><tr><td>4000018</td><td>Raisin Ranch</td><td>raisins</td><td>Asia</td><td>Kabul</td><td>Chicken Street</td><td>XXL</td><td>69.1763</td><td>34.521</td><td>Y</td><td>bfaa5e74c298d71cf0d035a20644361602075b8b7a33f46fa9dbd3563206a152</td></tr><tr><td>4000019</td><td>Cardamom Cove</td><td>cardamom</td><td>Asia</td><td>Kozhikode</td><td>Mappila Bay</td><td>XL</td><td>75.7804</td><td>11.2588</td><td>Y</td><td>cadeb8f080ad755357856e8c7bde19a670057d49ac3d2ca1dc50d347041e3d7f</td></tr><tr><td>4000020</td><td>Clove Cliffs</td><td>cloves</td><td>Africa</td><td>Zanzibar</td><td>Stone Town</td><td>S</td><td>39.1918</td><td>-6.167</td><td>Y</td><td>5c31888c164b100c4af0ede198214807ea86dcde4b229ad70d7f1fa3b518534c</td></tr><tr><td>4000021</td><td>Cocoa Crops</td><td>cocoa butter</td><td>South America</td><td>Caracas</td><td>Las Mercedes</td><td>M</td><td>-66.9036</td><td>10.4806</td><td>Y</td><td>043568b243af138d5029b9feb30635eb71f40e8aa30b0f4cc8f5d14f78f1cf63</td></tr><tr><td>4000022</td><td>Poppy Peaks</td><td>poppy seeds</td><td>Europe</td><td>Krakow</td><td>Kazimierz</td><td>S</td><td>19.9368</td><td>50.0647</td><td>Y</td><td>29615452d067ea16096d2061788bbd51a75b01c975a86aebf5dfe2d21a2f274f</td></tr><tr><td>4000023</td><td>Fennel Fields</td><td>fennel seeds</td><td>Europe</td><td>Florence</td><td>Santo Spirito</td><td>L</td><td>11.2558</td><td>43.7695</td><td>Y</td><td>632cc73f404913cb5d284f4bfb0c40bf4637be189e91085de194b404e6e92bb8</td></tr><tr><td>4000024</td><td>Anise Acres</td><td>anise</td><td>Asia</td><td>Izmir</td><td>Alsancak</td><td>XL</td><td>27.1384</td><td>38.4237</td><td>Y</td><td>9ed8c627d37707874f8c70586fc83991e26db6d7f9c8a6473e80e3c87192d1f6</td></tr><tr><td>4000025</td><td>Nutmeg Nirvana</td><td>nutmeg</td><td>Asia</td><td>Banda Aceh</td><td>Peunayong</td><td>M</td><td>95.3198</td><td>5.5577</td><td>Y</td><td>9aeb18dc8f6e19a8354a0fdccf7d8d72716e85a5cf85024c9835fe38482cb8c6</td></tr><tr><td>4000026</td><td>Mace Meadows</td><td>mace</td><td>Asia</td><td>Ambon</td><td>Namalatu</td><td>S</td><td>128.1753</td><td>-3.6953</td><td>Y</td><td>3f8e9141bbf08b8b5cf1f60463843d321ee2798b34f2403a7e81024a6e0acaf4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         4000000,
         "Cacao Wonders",
         "cacao",
         "South America",
         "Guayaquil",
         "Las Peñas",
         "M",
         -79.8974,
         -2.1791,
         "Y",
         "19e37f9076246ae6ffb69e9be7ed2073d94656fb55aadc78ee09807ffd86ff36"
        ],
        [
         4000001,
         "Coconut Grove",
         "coconut",
         "Asia",
         "Manila",
         "Intramuros",
         "S",
         121.0221,
         14.6042,
         "Y",
         "dd8379700727ada5448616395cee167291078ba63a10ae62f20ea3708b19004a"
        ],
        [
         4000002,
         "Almond Delights",
         "almonds",
         "Europe",
         "Valencia",
         "Ruzafa",
         "L",
         -0.3762,
         39.4699,
         "Y",
         "2618ef1944d020b72ae94ce0343b054c4818855400650829c903c0a0bec5004c"
        ],
        [
         4000003,
         "Sugar Cane Harvest",
         "cane sugar",
         "South America",
         "Sao Paulo",
         "Vila Madalena",
         "XL",
         -46.6333,
         -23.5489,
         "Y",
         "98327b4f32a42f832fa286d6fd6c96ba0734010fc2fdb4cb3ac62aa321f2b06c"
        ],
        [
         4000004,
         "Vanilla Valley",
         "vanilla",
         "North America",
         "Mexico City",
         "Roma Norte",
         "M",
         -99.1332,
         19.4326,
         "Y",
         "e6d6a60d578c4b2907df80d8b136e67370b976e9a0754f68a4dd914058b61839"
        ],
        [
         4000005,
         "Pecan Pleasures",
         "pecans",
         "North America",
         "Atlanta",
         "Virginia-Highland",
         "S",
         -84.3888,
         33.749,
         "Y",
         "eb24f9d60f76123cd4744b9bff18c66b5c1babd2a3950faef75a353837c96dd0"
        ],
        [
         4000006,
         "Hazelnut Haven",
         "hazelnuts",
         "Europe",
         "Istanbul",
         "Kadıköy",
         "XXL",
         28.9784,
         41.0082,
         "Y",
         "432fd8759f3212a0b0fb1b90a232b03f65ff6883d1cf4247f71366737d55d732"
        ],
        [
         4000007,
         "Cinnamon Spice",
         "cinnamon",
         "Asia",
         "Colombo",
         "Galle Face Green",
         "L",
         79.8612,
         6.9271,
         "Y",
         "5bb8fa1ebe1c59e25cf6022593adba52ad61c9fb79d47eccd81d402afa43597c"
        ],
        [
         4000008,
         "Cashew Corner",
         "cashews",
         "Asia",
         "Goa",
         "Anjuna Beach",
         "XL",
         73.8067,
         15.3173,
         "Y",
         "edfa576dd2e0c945b119604b91e8d4680a64e069945367ff633b7ae2d72be8fe"
        ],
        [
         4000009,
         "Maple Monarch",
         "maple syrup",
         "North America",
         "Montreal",
         "Plateau Mont-Royal",
         "M",
         -73.5673,
         45.5017,
         "Y",
         "2de584aa231a74956a5ac5dfd69a06aab3050410b11f8c89c095b95b4feaee24"
        ],
        [
         4000010,
         "Pistachio Palace",
         "pistachios",
         "Asia",
         "Tehran",
         "Tajrish Bazaar",
         "S",
         51.4215,
         35.7106,
         "Y",
         "a7afea4951a2aa7bf2b6e58ecd8b712c44fd0e9d659004e47a1a9bc03ad6e356"
        ],
        [
         4000011,
         "Oat Oasis",
         "oats",
         "Europe",
         "Edinburgh",
         "Stockbridge",
         "L",
         -3.1883,
         55.9533,
         "Y",
         "6ef1ac8bb1bd9456c4817028fc2d038187bc3577e9a62a0f1386d949b127ba24"
        ],
        [
         4000012,
         "Coffee Collective",
         "coffee",
         "South America",
         "Medellin",
         "El Poblado",
         "XXL",
         -75.5638,
         6.2518,
         "Y",
         "4b60aa6f629df63cc6d694ea0213d6de5a03cd5c849243b953474ef81a2199c3"
        ],
        [
         4000013,
         "Ginger Gems",
         "ginger",
         "Asia",
         "Chennai",
         "Mylapore",
         "M",
         80.2707,
         13.0827,
         "Y",
         "d0fdd754477596380fd0c9b3bfb4f3b5b3f184b4cc527131e3d51185e19ed686"
        ],
        [
         4000014,
         "Molasses Mills",
         "molasses",
         "Central America",
         "Havana",
         "Vedado",
         "XL",
         -82.3665,
         23.1136,
         "Y",
         "fed099934e5785df6b1b7ffa7eb61a9cd02fefa6c13aac2da93a82348d68f361"
        ],
        [
         4000015,
         "Honey Hives",
         "honey",
         "Africa",
         "Addis Ababa",
         "Piazza",
         "S",
         38.7575,
         9.0084,
         "Y",
         "10d4b53538fc1b2296542295ba141067f8c7198424daa003a9d434c2a262da83"
        ],
        [
         4000016,
         "Sesame Seeds",
         "sesame seeds",
         "Asia",
         "Yangon",
         "Chinatown",
         "L",
         96.1611,
         16.8409,
         "Y",
         "424d71a18a50a22e7e4afb5aa3546bb1ae81cba3a4ecd8981d30ca20b283c65c"
        ],
        [
         4000017,
         "Peanut Plantation",
         "peanuts",
         "Africa",
         "Kano",
         "Kurmi Market",
         "M",
         8.5167,
         12.0022,
         "Y",
         "06a6e7680bcc613529013a1c61f185f36216600157af9706c3acb33d17c307e5"
        ],
        [
         4000018,
         "Raisin Ranch",
         "raisins",
         "Asia",
         "Kabul",
         "Chicken Street",
         "XXL",
         69.1763,
         34.521,
         "Y",
         "bfaa5e74c298d71cf0d035a20644361602075b8b7a33f46fa9dbd3563206a152"
        ],
        [
         4000019,
         "Cardamom Cove",
         "cardamom",
         "Asia",
         "Kozhikode",
         "Mappila Bay",
         "XL",
         75.7804,
         11.2588,
         "Y",
         "cadeb8f080ad755357856e8c7bde19a670057d49ac3d2ca1dc50d347041e3d7f"
        ],
        [
         4000020,
         "Clove Cliffs",
         "cloves",
         "Africa",
         "Zanzibar",
         "Stone Town",
         "S",
         39.1918,
         -6.167,
         "Y",
         "5c31888c164b100c4af0ede198214807ea86dcde4b229ad70d7f1fa3b518534c"
        ],
        [
         4000021,
         "Cocoa Crops",
         "cocoa butter",
         "South America",
         "Caracas",
         "Las Mercedes",
         "M",
         -66.9036,
         10.4806,
         "Y",
         "043568b243af138d5029b9feb30635eb71f40e8aa30b0f4cc8f5d14f78f1cf63"
        ],
        [
         4000022,
         "Poppy Peaks",
         "poppy seeds",
         "Europe",
         "Krakow",
         "Kazimierz",
         "S",
         19.9368,
         50.0647,
         "Y",
         "29615452d067ea16096d2061788bbd51a75b01c975a86aebf5dfe2d21a2f274f"
        ],
        [
         4000023,
         "Fennel Fields",
         "fennel seeds",
         "Europe",
         "Florence",
         "Santo Spirito",
         "L",
         11.2558,
         43.7695,
         "Y",
         "632cc73f404913cb5d284f4bfb0c40bf4637be189e91085de194b404e6e92bb8"
        ],
        [
         4000024,
         "Anise Acres",
         "anise",
         "Asia",
         "Izmir",
         "Alsancak",
         "XL",
         27.1384,
         38.4237,
         "Y",
         "9ed8c627d37707874f8c70586fc83991e26db6d7f9c8a6473e80e3c87192d1f6"
        ],
        [
         4000025,
         "Nutmeg Nirvana",
         "nutmeg",
         "Asia",
         "Banda Aceh",
         "Peunayong",
         "M",
         95.3198,
         5.5577,
         "Y",
         "9aeb18dc8f6e19a8354a0fdccf7d8d72716e85a5cf85024c9835fe38482cb8c6"
        ],
        [
         4000026,
         "Mace Meadows",
         "mace",
         "Asia",
         "Ambon",
         "Namalatu",
         "S",
         128.1753,
         -3.6953,
         "Y",
         "3f8e9141bbf08b8b5cf1f60463843d321ee2798b34f2403a7e81024a6e0acaf4"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "supplierID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ingredient",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "continent",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "district",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "longitude",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "latitude",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "approved",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "RowHash",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a354c24a-528b-4959-9a48-cffb7d2f5b46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add three new columns to source :\n",
    "# 1. 'IndCurrent': Set to 1 for all rows, indicating the current/active record.\n",
    "# 2. 'CreatedDate': Set to the current timestamp, representing when the record was created.\n",
    "# 3. 'ModifiedDate': Set to the current timestamp, representing when the record was last modified.\n",
    "source = source.withColumn(\"IndCurrent\", F.lit(1)) \\\n",
    "    .withColumn(\"CreatedDate\", F.current_timestamp()) \\\n",
    "    .withColumn(\"ModifiedDate\", F.current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9361455d-368b-4687-8adc-ce449409c8d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "# Define the window specification\n",
    "window_spec = Window.orderBy(F.monotonically_increasing_id())\n",
    "\n",
    "# Add a row number column based on the window specification\n",
    "source = source.withColumn(\"storage_id\", F.row_number().over(window_spec))\n",
    "\n",
    "first_cols = [\"storage_id\"]\n",
    "other_cols = [col for col in source.columns if col not in first_cols]\n",
    "source = source.select(first_cols + other_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35881609-fdd0-4f3b-8615-39468fc96c81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#table_name= 'workspace.target.sales2'\n",
    "#source.write.format('delta').mode('append').saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e0c1487-b422-4bcf-9ac5-d0a21e313d03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "Source = source.withColumn(\n",
    "    \"ingredient\",\n",
    "    when(col(\"supplierID\") == \"4000000\", \"Mumbai\").otherwise(col(\"ingredient\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "245f66af-c1b1-495d-8a09-ce9f75160c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols= Source.columns\n",
    "cols = cols[1:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1210314e-3152-4541-8a7a-20852ae0797c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Concatenate all columns in 'source' DataFrame into 'RowHash'\n",
    "SourceDf = Source.withColumn('RowHash', F.sha2(F.concat_ws('', *cols), 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ac3c11d-8333-415e-b3ad-94a22f207433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "SourceDf=SourceDf.drop(\"storage_Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8137e742-5aa7-487c-abd1-5f98e2e884e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>supplierID</th><th>name</th><th>ingredient</th><th>continent</th><th>city</th><th>district</th><th>size</th><th>longitude</th><th>latitude</th><th>approved</th><th>RowHash</th><th>IndCurrent</th><th>CreatedDate</th><th>ModifiedDate</th><th>storage_id</th><th>Flag</th></tr></thead><tbody><tr><td>4000025</td><td>Nutmeg Nirvana</td><td>nutmeg</td><td>Asia</td><td>Banda Aceh</td><td>Peunayong</td><td>M</td><td>95.3198</td><td>5.5577</td><td>Y</td><td>9aeb18dc8f6e19a8354a0fdccf7d8d72716e85a5cf85024c9835fe38482cb8c6</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000012</td><td>Coffee Collective</td><td>coffee</td><td>South America</td><td>Medellin</td><td>El Poblado</td><td>XXL</td><td>-75.5638</td><td>6.2518</td><td>Y</td><td>4b60aa6f629df63cc6d694ea0213d6de5a03cd5c849243b953474ef81a2199c3</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000011</td><td>Oat Oasis</td><td>oats</td><td>Europe</td><td>Edinburgh</td><td>Stockbridge</td><td>L</td><td>-3.1883</td><td>55.9533</td><td>Y</td><td>6ef1ac8bb1bd9456c4817028fc2d038187bc3577e9a62a0f1386d949b127ba24</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000018</td><td>Raisin Ranch</td><td>raisins</td><td>Asia</td><td>Kabul</td><td>Chicken Street</td><td>XXL</td><td>69.1763</td><td>34.521</td><td>Y</td><td>bfaa5e74c298d71cf0d035a20644361602075b8b7a33f46fa9dbd3563206a152</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000023</td><td>Fennel Fields</td><td>fennel seeds</td><td>Europe</td><td>Florence</td><td>Santo Spirito</td><td>L</td><td>11.2558</td><td>43.7695</td><td>Y</td><td>632cc73f404913cb5d284f4bfb0c40bf4637be189e91085de194b404e6e92bb8</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000022</td><td>Poppy Peaks</td><td>poppy seeds</td><td>Europe</td><td>Krakow</td><td>Kazimierz</td><td>S</td><td>19.9368</td><td>50.0647</td><td>Y</td><td>29615452d067ea16096d2061788bbd51a75b01c975a86aebf5dfe2d21a2f274f</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000002</td><td>Almond Delights</td><td>almonds</td><td>Europe</td><td>Valencia</td><td>Ruzafa</td><td>L</td><td>-0.3762</td><td>39.4699</td><td>Y</td><td>2618ef1944d020b72ae94ce0343b054c4818855400650829c903c0a0bec5004c</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000014</td><td>Molasses Mills</td><td>molasses</td><td>Central America</td><td>Havana</td><td>Vedado</td><td>XL</td><td>-82.3665</td><td>23.1136</td><td>Y</td><td>fed099934e5785df6b1b7ffa7eb61a9cd02fefa6c13aac2da93a82348d68f361</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000005</td><td>Pecan Pleasures</td><td>pecans</td><td>North America</td><td>Atlanta</td><td>Virginia-Highland</td><td>S</td><td>-84.3888</td><td>33.749</td><td>Y</td><td>eb24f9d60f76123cd4744b9bff18c66b5c1babd2a3950faef75a353837c96dd0</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000001</td><td>Coconut Grove</td><td>coconut</td><td>Asia</td><td>Manila</td><td>Intramuros</td><td>S</td><td>121.0221</td><td>14.6042</td><td>Y</td><td>dd8379700727ada5448616395cee167291078ba63a10ae62f20ea3708b19004a</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000019</td><td>Cardamom Cove</td><td>cardamom</td><td>Asia</td><td>Kozhikode</td><td>Mappila Bay</td><td>XL</td><td>75.7804</td><td>11.2588</td><td>Y</td><td>cadeb8f080ad755357856e8c7bde19a670057d49ac3d2ca1dc50d347041e3d7f</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000000</td><td>Cacao Wonders</td><td>Mumbai</td><td>South America</td><td>Guayaquil</td><td>Las Peñas</td><td>M</td><td>-79.8974</td><td>-2.1791</td><td>Y</td><td>cc1bc6e8d66c1c16c8040dd8685097372d56891f2b2bd5d3dc45b0e6575eb406</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000024</td><td>Anise Acres</td><td>anise</td><td>Asia</td><td>Izmir</td><td>Alsancak</td><td>XL</td><td>27.1384</td><td>38.4237</td><td>Y</td><td>9ed8c627d37707874f8c70586fc83991e26db6d7f9c8a6473e80e3c87192d1f6</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000004</td><td>Vanilla Valley</td><td>vanilla</td><td>North America</td><td>Mexico City</td><td>Roma Norte</td><td>M</td><td>-99.1332</td><td>19.4326</td><td>Y</td><td>e6d6a60d578c4b2907df80d8b136e67370b976e9a0754f68a4dd914058b61839</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000020</td><td>Clove Cliffs</td><td>cloves</td><td>Africa</td><td>Zanzibar</td><td>Stone Town</td><td>S</td><td>39.1918</td><td>-6.167</td><td>Y</td><td>5c31888c164b100c4af0ede198214807ea86dcde4b229ad70d7f1fa3b518534c</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000016</td><td>Sesame Seeds</td><td>sesame seeds</td><td>Asia</td><td>Yangon</td><td>Chinatown</td><td>L</td><td>96.1611</td><td>16.8409</td><td>Y</td><td>424d71a18a50a22e7e4afb5aa3546bb1ae81cba3a4ecd8981d30ca20b283c65c</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000003</td><td>Sugar Cane Harvest</td><td>cane sugar</td><td>South America</td><td>Sao Paulo</td><td>Vila Madalena</td><td>XL</td><td>-46.6333</td><td>-23.5489</td><td>Y</td><td>98327b4f32a42f832fa286d6fd6c96ba0734010fc2fdb4cb3ac62aa321f2b06c</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000007</td><td>Cinnamon Spice</td><td>cinnamon</td><td>Asia</td><td>Colombo</td><td>Galle Face Green</td><td>L</td><td>79.8612</td><td>6.9271</td><td>Y</td><td>5bb8fa1ebe1c59e25cf6022593adba52ad61c9fb79d47eccd81d402afa43597c</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000006</td><td>Hazelnut Haven</td><td>hazelnuts</td><td>Europe</td><td>Istanbul</td><td>Kadıköy</td><td>XXL</td><td>28.9784</td><td>41.0082</td><td>Y</td><td>432fd8759f3212a0b0fb1b90a232b03f65ff6883d1cf4247f71366737d55d732</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000017</td><td>Peanut Plantation</td><td>peanuts</td><td>Africa</td><td>Kano</td><td>Kurmi Market</td><td>M</td><td>8.5167</td><td>12.0022</td><td>Y</td><td>06a6e7680bcc613529013a1c61f185f36216600157af9706c3acb33d17c307e5</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000026</td><td>Mace Meadows</td><td>mace</td><td>Asia</td><td>Ambon</td><td>Namalatu</td><td>S</td><td>128.1753</td><td>-3.6953</td><td>Y</td><td>3f8e9141bbf08b8b5cf1f60463843d321ee2798b34f2403a7e81024a6e0acaf4</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000013</td><td>Ginger Gems</td><td>ginger</td><td>Asia</td><td>Chennai</td><td>Mylapore</td><td>M</td><td>80.2707</td><td>13.0827</td><td>Y</td><td>d0fdd754477596380fd0c9b3bfb4f3b5b3f184b4cc527131e3d51185e19ed686</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000008</td><td>Cashew Corner</td><td>cashews</td><td>Asia</td><td>Goa</td><td>Anjuna Beach</td><td>XL</td><td>73.8067</td><td>15.3173</td><td>Y</td><td>edfa576dd2e0c945b119604b91e8d4680a64e069945367ff633b7ae2d72be8fe</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000015</td><td>Honey Hives</td><td>honey</td><td>Africa</td><td>Addis Ababa</td><td>Piazza</td><td>S</td><td>38.7575</td><td>9.0084</td><td>Y</td><td>10d4b53538fc1b2296542295ba141067f8c7198424daa003a9d434c2a262da83</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000021</td><td>Cocoa Crops</td><td>cocoa butter</td><td>South America</td><td>Caracas</td><td>Las Mercedes</td><td>M</td><td>-66.9036</td><td>10.4806</td><td>Y</td><td>043568b243af138d5029b9feb30635eb71f40e8aa30b0f4cc8f5d14f78f1cf63</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000009</td><td>Maple Monarch</td><td>maple syrup</td><td>North America</td><td>Montreal</td><td>Plateau Mont-Royal</td><td>M</td><td>-73.5673</td><td>45.5017</td><td>Y</td><td>2de584aa231a74956a5ac5dfd69a06aab3050410b11f8c89c095b95b4feaee24</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000010</td><td>Pistachio Palace</td><td>pistachios</td><td>Asia</td><td>Tehran</td><td>Tajrish Bazaar</td><td>S</td><td>51.4215</td><td>35.7106</td><td>Y</td><td>a7afea4951a2aa7bf2b6e58ecd8b712c44fd0e9d659004e47a1a9bc03ad6e356</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>28</td><td>New</td></tr><tr><td>4000000</td><td>Cacao Wonders</td><td>Mumbai</td><td>South America</td><td>Guayaquil</td><td>Las Peñas</td><td>M</td><td>-79.8974</td><td>-2.1791</td><td>Y</td><td>cc1bc6e8d66c1c16c8040dd8685097372d56891f2b2bd5d3dc45b0e6575eb406</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>1</td><td>New</td></tr><tr><td>4000000</td><td>Cacao Wonders</td><td>Mumbai</td><td>South America</td><td>Guayaquil</td><td>Las Peñas</td><td>M</td><td>-79.8974</td><td>-2.1791</td><td>Y</td><td>cc1bc6e8d66c1c16c8040dd8685097372d56891f2b2bd5d3dc45b0e6575eb406</td><td>1</td><td>2025-07-09T17:07:59.017Z</td><td>2025-07-09T17:07:59.017Z</td><td>1</td><td>New</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         4000025,
         "Nutmeg Nirvana",
         "nutmeg",
         "Asia",
         "Banda Aceh",
         "Peunayong",
         "M",
         95.3198,
         5.5577,
         "Y",
         "9aeb18dc8f6e19a8354a0fdccf7d8d72716e85a5cf85024c9835fe38482cb8c6",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000012,
         "Coffee Collective",
         "coffee",
         "South America",
         "Medellin",
         "El Poblado",
         "XXL",
         -75.5638,
         6.2518,
         "Y",
         "4b60aa6f629df63cc6d694ea0213d6de5a03cd5c849243b953474ef81a2199c3",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000011,
         "Oat Oasis",
         "oats",
         "Europe",
         "Edinburgh",
         "Stockbridge",
         "L",
         -3.1883,
         55.9533,
         "Y",
         "6ef1ac8bb1bd9456c4817028fc2d038187bc3577e9a62a0f1386d949b127ba24",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000018,
         "Raisin Ranch",
         "raisins",
         "Asia",
         "Kabul",
         "Chicken Street",
         "XXL",
         69.1763,
         34.521,
         "Y",
         "bfaa5e74c298d71cf0d035a20644361602075b8b7a33f46fa9dbd3563206a152",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000023,
         "Fennel Fields",
         "fennel seeds",
         "Europe",
         "Florence",
         "Santo Spirito",
         "L",
         11.2558,
         43.7695,
         "Y",
         "632cc73f404913cb5d284f4bfb0c40bf4637be189e91085de194b404e6e92bb8",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000022,
         "Poppy Peaks",
         "poppy seeds",
         "Europe",
         "Krakow",
         "Kazimierz",
         "S",
         19.9368,
         50.0647,
         "Y",
         "29615452d067ea16096d2061788bbd51a75b01c975a86aebf5dfe2d21a2f274f",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000002,
         "Almond Delights",
         "almonds",
         "Europe",
         "Valencia",
         "Ruzafa",
         "L",
         -0.3762,
         39.4699,
         "Y",
         "2618ef1944d020b72ae94ce0343b054c4818855400650829c903c0a0bec5004c",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000014,
         "Molasses Mills",
         "molasses",
         "Central America",
         "Havana",
         "Vedado",
         "XL",
         -82.3665,
         23.1136,
         "Y",
         "fed099934e5785df6b1b7ffa7eb61a9cd02fefa6c13aac2da93a82348d68f361",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000005,
         "Pecan Pleasures",
         "pecans",
         "North America",
         "Atlanta",
         "Virginia-Highland",
         "S",
         -84.3888,
         33.749,
         "Y",
         "eb24f9d60f76123cd4744b9bff18c66b5c1babd2a3950faef75a353837c96dd0",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000001,
         "Coconut Grove",
         "coconut",
         "Asia",
         "Manila",
         "Intramuros",
         "S",
         121.0221,
         14.6042,
         "Y",
         "dd8379700727ada5448616395cee167291078ba63a10ae62f20ea3708b19004a",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000019,
         "Cardamom Cove",
         "cardamom",
         "Asia",
         "Kozhikode",
         "Mappila Bay",
         "XL",
         75.7804,
         11.2588,
         "Y",
         "cadeb8f080ad755357856e8c7bde19a670057d49ac3d2ca1dc50d347041e3d7f",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000000,
         "Cacao Wonders",
         "Mumbai",
         "South America",
         "Guayaquil",
         "Las Peñas",
         "M",
         -79.8974,
         -2.1791,
         "Y",
         "cc1bc6e8d66c1c16c8040dd8685097372d56891f2b2bd5d3dc45b0e6575eb406",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000024,
         "Anise Acres",
         "anise",
         "Asia",
         "Izmir",
         "Alsancak",
         "XL",
         27.1384,
         38.4237,
         "Y",
         "9ed8c627d37707874f8c70586fc83991e26db6d7f9c8a6473e80e3c87192d1f6",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000004,
         "Vanilla Valley",
         "vanilla",
         "North America",
         "Mexico City",
         "Roma Norte",
         "M",
         -99.1332,
         19.4326,
         "Y",
         "e6d6a60d578c4b2907df80d8b136e67370b976e9a0754f68a4dd914058b61839",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000020,
         "Clove Cliffs",
         "cloves",
         "Africa",
         "Zanzibar",
         "Stone Town",
         "S",
         39.1918,
         -6.167,
         "Y",
         "5c31888c164b100c4af0ede198214807ea86dcde4b229ad70d7f1fa3b518534c",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000016,
         "Sesame Seeds",
         "sesame seeds",
         "Asia",
         "Yangon",
         "Chinatown",
         "L",
         96.1611,
         16.8409,
         "Y",
         "424d71a18a50a22e7e4afb5aa3546bb1ae81cba3a4ecd8981d30ca20b283c65c",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000003,
         "Sugar Cane Harvest",
         "cane sugar",
         "South America",
         "Sao Paulo",
         "Vila Madalena",
         "XL",
         -46.6333,
         -23.5489,
         "Y",
         "98327b4f32a42f832fa286d6fd6c96ba0734010fc2fdb4cb3ac62aa321f2b06c",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000007,
         "Cinnamon Spice",
         "cinnamon",
         "Asia",
         "Colombo",
         "Galle Face Green",
         "L",
         79.8612,
         6.9271,
         "Y",
         "5bb8fa1ebe1c59e25cf6022593adba52ad61c9fb79d47eccd81d402afa43597c",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000006,
         "Hazelnut Haven",
         "hazelnuts",
         "Europe",
         "Istanbul",
         "Kadıköy",
         "XXL",
         28.9784,
         41.0082,
         "Y",
         "432fd8759f3212a0b0fb1b90a232b03f65ff6883d1cf4247f71366737d55d732",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000017,
         "Peanut Plantation",
         "peanuts",
         "Africa",
         "Kano",
         "Kurmi Market",
         "M",
         8.5167,
         12.0022,
         "Y",
         "06a6e7680bcc613529013a1c61f185f36216600157af9706c3acb33d17c307e5",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000026,
         "Mace Meadows",
         "mace",
         "Asia",
         "Ambon",
         "Namalatu",
         "S",
         128.1753,
         -3.6953,
         "Y",
         "3f8e9141bbf08b8b5cf1f60463843d321ee2798b34f2403a7e81024a6e0acaf4",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000013,
         "Ginger Gems",
         "ginger",
         "Asia",
         "Chennai",
         "Mylapore",
         "M",
         80.2707,
         13.0827,
         "Y",
         "d0fdd754477596380fd0c9b3bfb4f3b5b3f184b4cc527131e3d51185e19ed686",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000008,
         "Cashew Corner",
         "cashews",
         "Asia",
         "Goa",
         "Anjuna Beach",
         "XL",
         73.8067,
         15.3173,
         "Y",
         "edfa576dd2e0c945b119604b91e8d4680a64e069945367ff633b7ae2d72be8fe",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000015,
         "Honey Hives",
         "honey",
         "Africa",
         "Addis Ababa",
         "Piazza",
         "S",
         38.7575,
         9.0084,
         "Y",
         "10d4b53538fc1b2296542295ba141067f8c7198424daa003a9d434c2a262da83",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000021,
         "Cocoa Crops",
         "cocoa butter",
         "South America",
         "Caracas",
         "Las Mercedes",
         "M",
         -66.9036,
         10.4806,
         "Y",
         "043568b243af138d5029b9feb30635eb71f40e8aa30b0f4cc8f5d14f78f1cf63",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000009,
         "Maple Monarch",
         "maple syrup",
         "North America",
         "Montreal",
         "Plateau Mont-Royal",
         "M",
         -73.5673,
         45.5017,
         "Y",
         "2de584aa231a74956a5ac5dfd69a06aab3050410b11f8c89c095b95b4feaee24",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000010,
         "Pistachio Palace",
         "pistachios",
         "Asia",
         "Tehran",
         "Tajrish Bazaar",
         "S",
         51.4215,
         35.7106,
         "Y",
         "a7afea4951a2aa7bf2b6e58ecd8b712c44fd0e9d659004e47a1a9bc03ad6e356",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         28,
         "New"
        ],
        [
         4000000,
         "Cacao Wonders",
         "Mumbai",
         "South America",
         "Guayaquil",
         "Las Peñas",
         "M",
         -79.8974,
         -2.1791,
         "Y",
         "cc1bc6e8d66c1c16c8040dd8685097372d56891f2b2bd5d3dc45b0e6575eb406",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         1,
         "New"
        ],
        [
         4000000,
         "Cacao Wonders",
         "Mumbai",
         "South America",
         "Guayaquil",
         "Las Peñas",
         "M",
         -79.8974,
         -2.1791,
         "Y",
         "cc1bc6e8d66c1c16c8040dd8685097372d56891f2b2bd5d3dc45b0e6575eb406",
         1,
         "2025-07-09T17:07:59.017Z",
         "2025-07-09T17:07:59.017Z",
         1,
         "New"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "supplierID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ingredient",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "continent",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "district",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "longitude",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "latitude",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "approved",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "RowHash",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "IndCurrent",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "CreatedDate",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "ModifiedDate",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "storage_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Flag",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "TargetDf=spark.read.table(table_name).select(['supplierID','RowHash','storage_id']).withColumnRenamed('RowHash','TargetHash')\n",
    "SourceDf=SourceDf.join(TargetDf, on =['supplierID'], how='left').withColumn('Flag', F.when(col('TargetHash').isNull() | (col('TargetHash') != col('RowHash')), 'New').when(col('TargetHash') == col('RowHash'), 'NoChange').otherwise('Update'))\n",
    "# Drop the TargetHash column\n",
    "SourceDf=SourceDf.drop('TargetHash')\n",
    "SourceDf=SourceDf.filter(col(\"Flag\") == \"New\")\n",
    "SourceDf.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "947f6573-5d6b-4c34-8490-2177062594c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mUnsupportedOperationException\u001B[0m             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6559142975587852>, line 40\u001B[0m\n",
       "\u001B[1;32m     26\u001B[0m tgt \u001B[38;5;241m=\u001B[39m target_table\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtgt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Use column expressions (not strings) in merge condition\u001B[39;00m\n",
       "\u001B[1;32m     29\u001B[0m tgt\u001B[38;5;241m.\u001B[39mmerge(\n",
       "\u001B[1;32m     30\u001B[0m     source\u001B[38;5;241m=\u001B[39msrc,\n",
       "\u001B[1;32m     31\u001B[0m     condition\u001B[38;5;241m=\u001B[39m(\n",
       "\u001B[1;32m     32\u001B[0m         (col(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtgt.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey_column\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m==\u001B[39m col(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msrc.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey_column\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)) \u001B[38;5;241m&\u001B[39m\n",
       "\u001B[1;32m     33\u001B[0m         (col(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtgt.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mis_current_column\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m==\u001B[39m lit(\u001B[38;5;241m1\u001B[39m))\n",
       "\u001B[1;32m     34\u001B[0m     )\n",
       "\u001B[1;32m     35\u001B[0m )\u001B[38;5;241m.\u001B[39mwhenMatchedUpdate(\n",
       "\u001B[1;32m     36\u001B[0m     condition\u001B[38;5;241m=\u001B[39mcol(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtgt.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhash_column\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m!=\u001B[39m col(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msrc.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhash_column\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m     37\u001B[0m     \u001B[38;5;28mset\u001B[39m\u001B[38;5;241m=\u001B[39m{\n",
       "\u001B[1;32m     38\u001B[0m         is_current_column: lit(\u001B[38;5;241m0\u001B[39m)\n",
       "\u001B[1;32m     39\u001B[0m     }\n",
       "\u001B[0;32m---> 40\u001B[0m )\u001B[38;5;241m.\u001B[39mwhenNotMatchedInsertAll()\u001B[38;5;241m.\u001B[39mexecute()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/delta/connect/tables.py:583\u001B[0m, in \u001B[0;36mDeltaMergeBuilder.execute\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    573\u001B[0m plan \u001B[38;5;241m=\u001B[39m MergeIntoTable(\n",
       "\u001B[1;32m    574\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_target,\n",
       "\u001B[1;32m    575\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_source,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    580\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_with_schema_evolution\n",
       "\u001B[1;32m    581\u001B[0m )\n",
       "\u001B[1;32m    582\u001B[0m df \u001B[38;5;241m=\u001B[39m DataFrame(plan, session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark)\n",
       "\u001B[0;32m--> 583\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(df\u001B[38;5;241m.\u001B[39mtoPandas())\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:1985\u001B[0m, in \u001B[0;36mDataFrame.toPandas\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1983\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtoPandas\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpandas.DataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   1984\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 1985\u001B[0m     pdf, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mto_pandas(query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mobservations)\n",
       "\u001B[1;32m   1986\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execution_info \u001B[38;5;241m=\u001B[39m ei\n",
       "\u001B[1;32m   1987\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pdf\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1117\u001B[0m, in \u001B[0;36mSparkConnectClient.to_pandas\u001B[0;34m(self, plan, observations)\u001B[0m\n",
       "\u001B[1;32m   1113\u001B[0m (self_destruct_conf,) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_config_with_defaults(\n",
       "\u001B[1;32m   1114\u001B[0m     (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.execution.arrow.pyspark.selfDestruct.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m   1115\u001B[0m )\n",
       "\u001B[1;32m   1116\u001B[0m self_destruct \u001B[38;5;241m=\u001B[39m cast(\u001B[38;5;28mstr\u001B[39m, self_destruct_conf)\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m-> 1117\u001B[0m table, schema, metrics, observed_metrics, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1118\u001B[0m     req, observations, self_destruct\u001B[38;5;241m=\u001B[39mself_destruct\n",
       "\u001B[1;32m   1119\u001B[0m )\n",
       "\u001B[1;32m   1120\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m table \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   1121\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1755\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   1752\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   1754\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 1755\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   1756\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   1757\u001B[0m     ):\n",
       "\u001B[1;32m   1758\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   1759\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1731\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   1729\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   1730\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1731\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2047\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2045\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2046\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2047\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2048\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   2049\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2149\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2134\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n",
       "\u001B[1;32m   2135\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython versions in the Spark Connect client and server are different. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2136\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo execute user-defined functions, client and server should have the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2145\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://docs.databricks.com/en/release-notes/serverless.html.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2146\u001B[0m                 )\n",
       "\u001B[1;32m   2147\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2149\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2150\u001B[0m                 info,\n",
       "\u001B[1;32m   2151\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2152\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2153\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2154\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2156\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2157\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mUnsupportedOperationException\u001B[0m: [DELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE] Cannot perform Merge as multiple source rows matched and attempted to modify the same\n",
       "target row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\n",
       "when multiple source rows match on the same target row, the result may be ambiguous\n",
       "as it is unclear which source row should be used to update or delete the matching\n",
       "target row. You can preprocess the source table to eliminate the possibility of\n",
       "multiple matches. Please refer to\n",
       "https://docs.databricks.com/delta/merge.html#merge-error\n",
       "\n",
       "JVM stacktrace:\n",
       "com.databricks.sql.transaction.tahoe.DeltaUnsupportedOperationException\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaErrorsEdge.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrorsEdge.scala:821)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaErrorsEdge.multipleSourceRowMatchingTargetRowInMergeException$(DeltaErrorsEdge.scala:816)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrors.scala:3952)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.throwErrorOnMultipleMatches(MergeIntoCommandBase.scala:306)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.throwErrorOnMultipleMatches$(MergeIntoCommandBase.scala:301)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.throwErrorOnMultipleMatches(MergeIntoCommandEdge.scala:67)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$findTouchedFilesForLowShuffleMerge$1(LowShuffleMergeExecutor.scala:667)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)\n",
       "\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n",
       "\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n",
       "\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:212)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)\n",
       "\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:442)\n",
       "\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:428)\n",
       "\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n",
       "\tat com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)\n",
       "\tat com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:55)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:314)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:301)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:55)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:177)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:407)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:405)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:55)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:176)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:55)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:165)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:154)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:55)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.findTouchedFilesForLowShuffleMerge(LowShuffleMergeExecutor.scala:478)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$runLowShuffleMerge$1(LowShuffleMergeExecutor.scala:231)\n",
       "\tat com.databricks.sql.transaction.tahoe.NoOpLowShuffleMergeExecutionObserver$.findTouchedFiles(LowShuffleMergeExecutionObserver.scala:87)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge(LowShuffleMergeExecutor.scala:228)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge$(LowShuffleMergeExecutor.scala:223)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.super$runLowShuffleMerge(MergeIntoCommandEdge.scala:405)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMergeInternal$1(MergeIntoCommandEdge.scala:405)\n",
       "\tat com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2432)\n",
       "\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n",
       "\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n",
       "\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:212)\n",
       "\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2432)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMergeInternal(MergeIntoCommandEdge.scala:392)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2(MergeIntoCommandEdge.scala:341)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2$adapted(MergeIntoCommandEdge.scala:294)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:302)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction(DeltaLogEdge.scala:156)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction$(DeltaLogEdge.scala:137)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog.withExistingOrNewTransaction(DeltaLog.scala:98)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$1(MergeIntoCommandEdge.scala:294)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)\n",
       "\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n",
       "\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n",
       "\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:212)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)\n",
       "\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:442)\n",
       "\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:428)\n",
       "\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n",
       "\tat com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)\n",
       "\tat com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:55)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:314)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:301)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:55)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:177)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:407)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:405)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:55)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:176)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:55)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:165)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:154)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:55)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMerge(MergeIntoCommandEdge.scala:292)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$run$1(MergeIntoCommandBase.scala:202)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:144)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:126)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runWithMaterializedSourceLostRetries(MergeIntoCommandEdge.scala:67)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run(MergeIntoCommandBase.scala:202)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run$(MergeIntoCommandBase.scala:171)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runCommand(MergeIntoCommandEdge.scala:75)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runFunc$1(DeltaDMLCommandEdge.scala:72)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.$anonfun$run$1(DeltaDMLCommandEdge.scala:91)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession(DMLUtils.scala:128)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession$(DMLUtils.scala:121)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runWithClonedSparkSession(DeltaDMLCommandEdge.scala:55)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.run(DeltaDMLCommandEdge.scala:91)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.run(MergeIntoCommandEdge.scala:72)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:441)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:441)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:440)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:477)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:413)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:776)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:343)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:212)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:713)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:436)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1299)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:432)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:369)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:430)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:494)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:489)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:489)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:489)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:325)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1683)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1744)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:330)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$7(Dataset.scala:236)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:230)\n",
       "\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:110)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:404)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:313)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
       "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "UnsupportedOperationException",
        "evalue": "[DELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE] Cannot perform Merge as multiple source rows matched and attempted to modify the same\ntarget row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\nwhen multiple source rows match on the same target row, the result may be ambiguous\nas it is unclear which source row should be used to update or delete the matching\ntarget row. You can preprocess the source table to eliminate the possibility of\nmultiple matches. Please refer to\nhttps://docs.databricks.com/delta/merge.html#merge-error\n\nJVM stacktrace:\ncom.databricks.sql.transaction.tahoe.DeltaUnsupportedOperationException\n\tat com.databricks.sql.transaction.tahoe.DeltaErrorsEdge.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrorsEdge.scala:821)\n\tat com.databricks.sql.transaction.tahoe.DeltaErrorsEdge.multipleSourceRowMatchingTargetRowInMergeException$(DeltaErrorsEdge.scala:816)\n\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrors.scala:3952)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.throwErrorOnMultipleMatches(MergeIntoCommandBase.scala:306)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.throwErrorOnMultipleMatches$(MergeIntoCommandBase.scala:301)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.throwErrorOnMultipleMatches(MergeIntoCommandEdge.scala:67)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$findTouchedFilesForLowShuffleMerge$1(LowShuffleMergeExecutor.scala:667)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:212)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:442)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:428)\n\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n\tat com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)\n\tat com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:314)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:301)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:177)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:407)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:405)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:176)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:175)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:165)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:154)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.findTouchedFilesForLowShuffleMerge(LowShuffleMergeExecutor.scala:478)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$runLowShuffleMerge$1(LowShuffleMergeExecutor.scala:231)\n\tat com.databricks.sql.transaction.tahoe.NoOpLowShuffleMergeExecutionObserver$.findTouchedFiles(LowShuffleMergeExecutionObserver.scala:87)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge(LowShuffleMergeExecutor.scala:228)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge$(LowShuffleMergeExecutor.scala:223)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.super$runLowShuffleMerge(MergeIntoCommandEdge.scala:405)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMergeInternal$1(MergeIntoCommandEdge.scala:405)\n\tat com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2432)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:212)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2432)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMergeInternal(MergeIntoCommandEdge.scala:392)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2(MergeIntoCommandEdge.scala:341)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2$adapted(MergeIntoCommandEdge.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:302)\n\tat com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction(DeltaLogEdge.scala:156)\n\tat com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction$(DeltaLogEdge.scala:137)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.withExistingOrNewTransaction(DeltaLog.scala:98)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$1(MergeIntoCommandEdge.scala:294)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:212)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:442)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:428)\n\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n\tat com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)\n\tat com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:314)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:301)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:177)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:407)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:405)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:176)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:175)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:165)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:154)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMerge(MergeIntoCommandEdge.scala:292)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$run$1(MergeIntoCommandBase.scala:202)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:144)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:126)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runWithMaterializedSourceLostRetries(MergeIntoCommandEdge.scala:67)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run(MergeIntoCommandBase.scala:202)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run$(MergeIntoCommandBase.scala:171)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runCommand(MergeIntoCommandEdge.scala:75)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runFunc$1(DeltaDMLCommandEdge.scala:72)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.$anonfun$run$1(DeltaDMLCommandEdge.scala:91)\n\tat com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession(DMLUtils.scala:128)\n\tat com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession$(DMLUtils.scala:121)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runWithClonedSparkSession(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.run(DeltaDMLCommandEdge.scala:91)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.run(MergeIntoCommandEdge.scala:72)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:441)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:441)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:440)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:477)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:413)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:776)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:343)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:212)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:713)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:436)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1299)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:432)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:369)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:430)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:494)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:489)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:489)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:489)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:325)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1683)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1744)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:330)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$7(Dataset.scala:236)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:230)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:110)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:404)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:313)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)"
       },
       "metadata": {
        "errorSummary": "[DELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE] Cannot perform Merge as multiple source rows matched and attempted to modify the same\ntarget row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\nwhen multiple source rows match on the same target row, the result may be ambiguous\nas it is unclear which source row should be used to update or delete the matching\ntarget row. You can preprocess the source table to eliminate the possibility of\nmultiple matches. Please refer to\nhttps://docs.databricks.com/delta/merge.html#merge-error SQLSTATE: 21506"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "DELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": "21506",
        "stackTrace": "com.databricks.sql.transaction.tahoe.DeltaUnsupportedOperationException\n\tat com.databricks.sql.transaction.tahoe.DeltaErrorsEdge.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrorsEdge.scala:821)\n\tat com.databricks.sql.transaction.tahoe.DeltaErrorsEdge.multipleSourceRowMatchingTargetRowInMergeException$(DeltaErrorsEdge.scala:816)\n\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrors.scala:3952)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.throwErrorOnMultipleMatches(MergeIntoCommandBase.scala:306)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.throwErrorOnMultipleMatches$(MergeIntoCommandBase.scala:301)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.throwErrorOnMultipleMatches(MergeIntoCommandEdge.scala:67)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$findTouchedFilesForLowShuffleMerge$1(LowShuffleMergeExecutor.scala:667)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:212)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:442)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:428)\n\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n\tat com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)\n\tat com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:314)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:301)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:177)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:407)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:405)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:176)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:175)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:165)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:154)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.findTouchedFilesForLowShuffleMerge(LowShuffleMergeExecutor.scala:478)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$runLowShuffleMerge$1(LowShuffleMergeExecutor.scala:231)\n\tat com.databricks.sql.transaction.tahoe.NoOpLowShuffleMergeExecutionObserver$.findTouchedFiles(LowShuffleMergeExecutionObserver.scala:87)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge(LowShuffleMergeExecutor.scala:228)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge$(LowShuffleMergeExecutor.scala:223)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.super$runLowShuffleMerge(MergeIntoCommandEdge.scala:405)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMergeInternal$1(MergeIntoCommandEdge.scala:405)\n\tat com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2432)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:212)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2432)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMergeInternal(MergeIntoCommandEdge.scala:392)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2(MergeIntoCommandEdge.scala:341)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2$adapted(MergeIntoCommandEdge.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:302)\n\tat com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction(DeltaLogEdge.scala:156)\n\tat com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction$(DeltaLogEdge.scala:137)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.withExistingOrNewTransaction(DeltaLog.scala:98)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$1(MergeIntoCommandEdge.scala:294)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:212)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:442)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:428)\n\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n\tat com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)\n\tat com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:314)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:301)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:177)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:407)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:405)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:176)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:175)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:165)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:154)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMerge(MergeIntoCommandEdge.scala:292)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$run$1(MergeIntoCommandBase.scala:202)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:144)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:126)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runWithMaterializedSourceLostRetries(MergeIntoCommandEdge.scala:67)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run(MergeIntoCommandBase.scala:202)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run$(MergeIntoCommandBase.scala:171)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runCommand(MergeIntoCommandEdge.scala:75)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runFunc$1(DeltaDMLCommandEdge.scala:72)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.$anonfun$run$1(DeltaDMLCommandEdge.scala:91)\n\tat com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession(DMLUtils.scala:128)\n\tat com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession$(DMLUtils.scala:121)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runWithClonedSparkSession(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.run(DeltaDMLCommandEdge.scala:91)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.run(MergeIntoCommandEdge.scala:72)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:441)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:441)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:440)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:477)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:413)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:776)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:343)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:212)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:713)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:436)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1299)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:432)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:369)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:430)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:494)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:489)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:489)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:489)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:325)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1683)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1744)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:330)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$7(Dataset.scala:236)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:230)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:110)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:404)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:313)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mUnsupportedOperationException\u001B[0m             Traceback (most recent call last)",
        "File \u001B[0;32m<command-6559142975587852>, line 40\u001B[0m\n\u001B[1;32m     26\u001B[0m tgt \u001B[38;5;241m=\u001B[39m target_table\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtgt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Use column expressions (not strings) in merge condition\u001B[39;00m\n\u001B[1;32m     29\u001B[0m tgt\u001B[38;5;241m.\u001B[39mmerge(\n\u001B[1;32m     30\u001B[0m     source\u001B[38;5;241m=\u001B[39msrc,\n\u001B[1;32m     31\u001B[0m     condition\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m     32\u001B[0m         (col(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtgt.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey_column\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m==\u001B[39m col(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msrc.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey_column\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)) \u001B[38;5;241m&\u001B[39m\n\u001B[1;32m     33\u001B[0m         (col(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtgt.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mis_current_column\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m==\u001B[39m lit(\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     34\u001B[0m     )\n\u001B[1;32m     35\u001B[0m )\u001B[38;5;241m.\u001B[39mwhenMatchedUpdate(\n\u001B[1;32m     36\u001B[0m     condition\u001B[38;5;241m=\u001B[39mcol(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtgt.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhash_column\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m!=\u001B[39m col(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msrc.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhash_column\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28mset\u001B[39m\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m     38\u001B[0m         is_current_column: lit(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     39\u001B[0m     }\n\u001B[0;32m---> 40\u001B[0m )\u001B[38;5;241m.\u001B[39mwhenNotMatchedInsertAll()\u001B[38;5;241m.\u001B[39mexecute()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/delta/connect/tables.py:583\u001B[0m, in \u001B[0;36mDeltaMergeBuilder.execute\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    573\u001B[0m plan \u001B[38;5;241m=\u001B[39m MergeIntoTable(\n\u001B[1;32m    574\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_target,\n\u001B[1;32m    575\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_source,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    580\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_with_schema_evolution\n\u001B[1;32m    581\u001B[0m )\n\u001B[1;32m    582\u001B[0m df \u001B[38;5;241m=\u001B[39m DataFrame(plan, session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark)\n\u001B[0;32m--> 583\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(df\u001B[38;5;241m.\u001B[39mtoPandas())\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:1985\u001B[0m, in \u001B[0;36mDataFrame.toPandas\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1983\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtoPandas\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpandas.DataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1984\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 1985\u001B[0m     pdf, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mto_pandas(query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mobservations)\n\u001B[1;32m   1986\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execution_info \u001B[38;5;241m=\u001B[39m ei\n\u001B[1;32m   1987\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pdf\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1117\u001B[0m, in \u001B[0;36mSparkConnectClient.to_pandas\u001B[0;34m(self, plan, observations)\u001B[0m\n\u001B[1;32m   1113\u001B[0m (self_destruct_conf,) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_config_with_defaults(\n\u001B[1;32m   1114\u001B[0m     (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.sql.execution.arrow.pyspark.selfDestruct.enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1115\u001B[0m )\n\u001B[1;32m   1116\u001B[0m self_destruct \u001B[38;5;241m=\u001B[39m cast(\u001B[38;5;28mstr\u001B[39m, self_destruct_conf)\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1117\u001B[0m table, schema, metrics, observed_metrics, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1118\u001B[0m     req, observations, self_destruct\u001B[38;5;241m=\u001B[39mself_destruct\n\u001B[1;32m   1119\u001B[0m )\n\u001B[1;32m   1120\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m table \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1121\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1755\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   1752\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1754\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1755\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   1756\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   1757\u001B[0m     ):\n\u001B[1;32m   1758\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1759\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1731\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   1729\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   1730\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1731\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2047\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2045\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2046\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2047\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2048\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2049\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2149\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2134\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[1;32m   2135\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython versions in the Spark Connect client and server are different. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2136\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo execute user-defined functions, client and server should have the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2145\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://docs.databricks.com/en/release-notes/serverless.html.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2146\u001B[0m                 )\n\u001B[1;32m   2147\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n\u001B[0;32m-> 2149\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2150\u001B[0m                 info,\n\u001B[1;32m   2151\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2152\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2153\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2154\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2156\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2157\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mUnsupportedOperationException\u001B[0m: [DELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE] Cannot perform Merge as multiple source rows matched and attempted to modify the same\ntarget row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\nwhen multiple source rows match on the same target row, the result may be ambiguous\nas it is unclear which source row should be used to update or delete the matching\ntarget row. You can preprocess the source table to eliminate the possibility of\nmultiple matches. Please refer to\nhttps://docs.databricks.com/delta/merge.html#merge-error\n\nJVM stacktrace:\ncom.databricks.sql.transaction.tahoe.DeltaUnsupportedOperationException\n\tat com.databricks.sql.transaction.tahoe.DeltaErrorsEdge.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrorsEdge.scala:821)\n\tat com.databricks.sql.transaction.tahoe.DeltaErrorsEdge.multipleSourceRowMatchingTargetRowInMergeException$(DeltaErrorsEdge.scala:816)\n\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrors.scala:3952)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.throwErrorOnMultipleMatches(MergeIntoCommandBase.scala:306)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.throwErrorOnMultipleMatches$(MergeIntoCommandBase.scala:301)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.throwErrorOnMultipleMatches(MergeIntoCommandEdge.scala:67)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$findTouchedFilesForLowShuffleMerge$1(LowShuffleMergeExecutor.scala:667)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:212)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:442)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:428)\n\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n\tat com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)\n\tat com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:314)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:301)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:177)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:407)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:405)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:176)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:175)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:165)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:154)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.findTouchedFilesForLowShuffleMerge(LowShuffleMergeExecutor.scala:478)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$runLowShuffleMerge$1(LowShuffleMergeExecutor.scala:231)\n\tat com.databricks.sql.transaction.tahoe.NoOpLowShuffleMergeExecutionObserver$.findTouchedFiles(LowShuffleMergeExecutionObserver.scala:87)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge(LowShuffleMergeExecutor.scala:228)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge$(LowShuffleMergeExecutor.scala:223)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.super$runLowShuffleMerge(MergeIntoCommandEdge.scala:405)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMergeInternal$1(MergeIntoCommandEdge.scala:405)\n\tat com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2432)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:212)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2432)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMergeInternal(MergeIntoCommandEdge.scala:392)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2(MergeIntoCommandEdge.scala:341)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2$adapted(MergeIntoCommandEdge.scala:294)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:302)\n\tat com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction(DeltaLogEdge.scala:156)\n\tat com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction$(DeltaLogEdge.scala:137)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.withExistingOrNewTransaction(DeltaLog.scala:98)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$1(MergeIntoCommandEdge.scala:294)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:212)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:442)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:428)\n\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n\tat com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)\n\tat com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:314)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:301)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:177)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:407)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:405)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:176)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:175)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:165)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:154)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMerge(MergeIntoCommandEdge.scala:292)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$run$1(MergeIntoCommandBase.scala:202)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:144)\n\tat com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:126)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runWithMaterializedSourceLostRetries(MergeIntoCommandEdge.scala:67)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run(MergeIntoCommandBase.scala:202)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run$(MergeIntoCommandBase.scala:171)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runCommand(MergeIntoCommandEdge.scala:75)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runFunc$1(DeltaDMLCommandEdge.scala:72)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.$anonfun$run$1(DeltaDMLCommandEdge.scala:91)\n\tat com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession(DMLUtils.scala:128)\n\tat com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession$(DMLUtils.scala:121)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runWithClonedSparkSession(DeltaDMLCommandEdge.scala:55)\n\tat com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.run(DeltaDMLCommandEdge.scala:91)\n\tat com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.run(MergeIntoCommandEdge.scala:72)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:441)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:441)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:216)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:440)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:477)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:413)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:776)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:343)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:212)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:713)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:436)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1299)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:432)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:369)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:430)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:494)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:489)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:489)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:489)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:325)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1683)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1744)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:330)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$7(Dataset.scala:236)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1457)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1457)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:230)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:110)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:404)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:313)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1450)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import current_timestamp, lit, col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "import uuid\n",
    "\n",
    "# Configuration\n",
    "table_name = \"workspace.target.sales2\"\n",
    "key_column = \"supplierID\"\n",
    "hash_column = \"RowHash\"\n",
    "is_current_column = \"IndCurrent\"\n",
    "surrogate_key_column = \"storage_id\"\n",
    "created_column = \"CreatedDate\"\n",
    "\n",
    "# Reference Delta table\n",
    "target_table = DeltaTable.forName(spark, table_name)\n",
    "\n",
    "# Add new columns to source DataFrame\n",
    "uuid_udf = udf(lambda: str(uuid.uuid4()), StringType())\n",
    "SourceDf = SourceDf \\\n",
    "    .withColumn(surrogate_key_column, uuid_udf()) \\\n",
    "    .withColumn(created_column, current_timestamp()) \\\n",
    "    .withColumn(is_current_column, lit(1))\n",
    "\n",
    "# Use aliases properly\n",
    "src = SourceDf.alias(\"src\")\n",
    "tgt = target_table.alias(\"tgt\")\n",
    "\n",
    "# Use column expressions (not strings) in merge condition\n",
    "tgt.merge(\n",
    "    source=src,\n",
    "    condition=(\n",
    "        (col(f\"tgt.{key_column}\") == col(f\"src.{key_column}\")) &\n",
    "        (col(f\"tgt.{is_current_column}\") == lit(1))\n",
    "    )\n",
    ").whenMatchedUpdate(\n",
    "    condition=col(f\"tgt.{hash_column}\") != col(f\"src.{hash_column}\"),\n",
    "    set={\n",
    "        is_current_column: lit(0)\n",
    "    }\n",
    ").whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdd7fbe8-7ad0-4d04-b8a0-495130b7c97d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SourceDf = SourceDf.drop('storage_id','Flag')\n",
    "max_storage_id = spark.sql(f\"select max(storage_id) as max_id from {table_name}\").first()['max_id']\n",
    "next_storage_id = 1 if not max_storage_id or max_storage_id == 0 else max_storage_id + 1\n",
    "\n",
    "SourceDf = SourceDf.withColumn('storage_id', lit(next_storage_id))\n",
    "SourceDf = SourceDf.withColumn('IndCurrent', lit(1))\n",
    "SourceDf.write.format('delta').mode('append').saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fb217eb-f648-47a3-aa8a-5434d2221a84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD_2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}